{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "import os,gc\n",
    "import datetime\n",
    "\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dropout, Dense, Embedding, SpatialDropout1D, concatenate, BatchNormalization, Flatten\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.losses import mean_squared_error as mse_loss\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAdam(Optimizer):\n",
    "    \"\"\"RAdam optimizer.\n",
    "    # Arguments\n",
    "        learning_rate: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        weight_decay: float >= 0. Weight decay for each param.\n",
    "        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n",
    "            algorithm from the paper \"On the Convergence of Adam and\n",
    "            Beyond\".\n",
    "        total_steps: int >= 0. Total number of training steps. Enable warmup by setting a positive value.\n",
    "        warmup_proportion: 0 < warmup_proportion < 1. The proportion of increasing steps.\n",
    "        min_lr: float >= 0. Minimum learning rate after warmup.\n",
    "    # References\n",
    "        - [Adam - A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980v8)\n",
    "        - [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)\n",
    "        - [On The Variance Of The Adaptive Learning Rate And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., weight_decay=0., amsgrad=False,\n",
    "                 total_steps=0, warmup_proportion=0.1, min_lr=0., **kwargs):\n",
    "        learning_rate = kwargs.pop('lr', learning_rate)\n",
    "        super(RAdam, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.learning_rate = K.variable(learning_rate, name='learning_rate')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            self.weight_decay = K.variable(weight_decay, name='weight_decay')\n",
    "            self.total_steps = K.variable(total_steps, name='total_steps')\n",
    "            self.warmup_proportion = K.variable(warmup_proportion, name='warmup_proportion')\n",
    "            self.min_lr = K.variable(min_lr, name='min_lr')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.initial_weight_decay = weight_decay\n",
    "        self.initial_total_steps = total_steps\n",
    "        self.amsgrad = amsgrad\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "\n",
    "        if self.initial_total_steps > 0:\n",
    "            warmup_steps = self.total_steps * self.warmup_proportion\n",
    "            decay_steps = K.maximum(self.total_steps - warmup_steps, 1)\n",
    "            decay_rate = (self.min_lr - lr) / decay_steps\n",
    "            lr = K.switch(\n",
    "                t <= warmup_steps,\n",
    "                lr * (t / warmup_steps),\n",
    "                lr + decay_rate * K.minimum(t - warmup_steps, decay_steps),\n",
    "            )\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='m_' + str(i)) for (i, p) in enumerate(params)]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='v_' + str(i)) for (i, p) in enumerate(params)]\n",
    "\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='vhat_' + str(i)) for (i, p) in enumerate(params)]\n",
    "        else:\n",
    "            vhats = [K.zeros(1, name='vhat_' + str(i)) for i in range(len(params))]\n",
    "\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        beta_1_t = K.pow(self.beta_1, t)\n",
    "        beta_2_t = K.pow(self.beta_2, t)\n",
    "\n",
    "        sma_inf = 2.0 / (1.0 - self.beta_2) - 1.0\n",
    "        sma_t = sma_inf - 2.0 * t * beta_2_t / (1.0 - beta_2_t)\n",
    "\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "\n",
    "            m_corr_t = m_t / (1.0 - beta_1_t)\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                v_corr_t = K.sqrt(vhat_t / (1.0 - beta_2_t))\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                v_corr_t = K.sqrt(v_t / (1.0 - beta_2_t))\n",
    "\n",
    "            r_t = K.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n",
    "                         (sma_t - 2.0) / (sma_inf - 2.0) *\n",
    "                         sma_inf / sma_t)\n",
    "\n",
    "            p_t = K.switch(sma_t >= 5, r_t * m_corr_t / (v_corr_t + self.epsilon), m_corr_t)\n",
    "\n",
    "            if self.initial_weight_decay > 0:\n",
    "                p_t += self.weight_decay * p\n",
    "\n",
    "            p_t = p - lr * p_t\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self.learning_rate\n",
    "\n",
    "    @lr.setter\n",
    "    def lr(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'learning_rate': float(K.get_value(self.learning_rate)),\n",
    "            'beta_1': float(K.get_value(self.beta_1)),\n",
    "            'beta_2': float(K.get_value(self.beta_2)),\n",
    "            'decay': float(K.get_value(self.decay)),\n",
    "            'weight_decay': float(K.get_value(self.weight_decay)),\n",
    "            'epsilon': self.epsilon,\n",
    "            'amsgrad': self.amsgrad,\n",
    "            'total_steps': float(K.get_value(self.total_steps)),\n",
    "            'warmup_proportion': float(K.get_value(self.warmup_proportion)),\n",
    "            'min_lr': float(K.get_value(self.min_lr)),\n",
    "        }\n",
    "        base_config = super(RAdam, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\"site_id\", \"building_id\", \"primary_use\", \"hour\", \"weekend\",  \"meter\"]\n",
    "drop_cols = [\"sea_level_pressure\", \"wind_speed\", 'wind_direction']\n",
    "numericals = [\"square_feet\", \"year_built\", \"air_temperature\", \"cloud_coverage\",\n",
    "              \"dew_temperature\", \"precip_depth_1_hr\", \"floor_count\"]\n",
    "feat_cols = categoricals + numericals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/kaggle/input/ashrae-energy-prediction'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_weather_dataset(weather_df):\n",
    "    \n",
    "    # Find Missing Dates\n",
    "    time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_date = datetime.datetime.strptime(weather_df['timestamp'].min(), time_format)\n",
    "    end_date = datetime.datetime.strptime(weather_df['timestamp'].max(), time_format)\n",
    "    total_hours = int(((end_date - start_date).total_seconds() + 3600) / 3600)\n",
    "    hours_list = [(end_date - datetime.timedelta(hours=x)).strftime(time_format) for x in range(total_hours)]\n",
    "\n",
    "    missing_hours = []\n",
    "    for site_id in range(16):\n",
    "        site_hours = np.array(weather_df[weather_df['site_id'] == site_id]['timestamp'])\n",
    "        new_rows = pd.DataFrame(np.setdiff1d(hours_list,site_hours), columns=['timestamp'])\n",
    "        new_rows['site_id'] = site_id\n",
    "        weather_df = pd.concat([weather_df,new_rows])\n",
    "\n",
    "        weather_df = weather_df.reset_index(drop=True)           \n",
    "\n",
    "    # Add new Features\n",
    "    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"timestamp\"])\n",
    "    weather_df[\"day\"] = weather_df[\"datetime\"].dt.day\n",
    "    weather_df[\"week\"] = weather_df[\"datetime\"].dt.week\n",
    "    weather_df[\"month\"] = weather_df[\"datetime\"].dt.month\n",
    "    \n",
    "    # Reset Index for Fast Update\n",
    "    weather_df = weather_df.set_index(['site_id', 'day', 'month'])\n",
    "\n",
    "    air_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id', 'day', 'month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\n",
    "    weather_df.update(air_temperature_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    cloud_coverage_filler = weather_df.groupby(['site_id', 'day', 'month'])['cloud_coverage'].mean()\n",
    "    # Step 2\n",
    "    cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method='ffill'),columns=[\"cloud_coverage\"])\n",
    "    weather_df.update(cloud_coverage_filler,overwrite=False)\n",
    "\n",
    "    due_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id', 'day', 'month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\n",
    "    weather_df.update(due_temperature_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    precip_depth_filler = weather_df.groupby(['site_id', 'day', 'month'])['precip_depth_1_hr'].mean()\n",
    "    # Step 2\n",
    "    precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method='ffill'),columns=['precip_depth_1_hr'])\n",
    "    weather_df.update(precip_depth_filler,overwrite=False)\n",
    "\n",
    "    weather_df = weather_df.reset_index()\n",
    "    weather_df = weather_df.drop(['datetime', 'day', 'week', 'month'],axis=1)\n",
    "        \n",
    "    return weather_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train = pd.read_csv(directory+'/weather_train.csv')\n",
    "weather_test = pd.read_csv(directory+'/weather_test.csv')\n",
    "weather = pd.concat([weather_train, weather_test], ignore_index=True)\n",
    "del weather_train, weather_test\n",
    "gc.collect()\n",
    "weather = fill_weather_dataset(weather)\n",
    "weather['timestamp'] = pd.to_datetime(weather['timestamp'])\n",
    "weather = weather.sort_values(['site_id', 'timestamp']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_ids_offsets = pd.DataFrame({'site_id': {0: 5,\n",
    "                                              1: 0,\n",
    "                                              2: 9,\n",
    "                                              3: 6,\n",
    "                                              4: 8,\n",
    "                                              5: 0,\n",
    "                                              6: 6,\n",
    "                                              7: 6,\n",
    "                                              8: 5,\n",
    "                                              9: 7,\n",
    "                                              10: 8,\n",
    "                                              11: 6,\n",
    "                                              12: 0,\n",
    "                                              13: 7,\n",
    "                                              14: 6,\n",
    "                                              15: 6}})\n",
    "weather['offset'] = weather['site_id'].map(site_ids_offsets['site_id'])\n",
    "# add offset\n",
    "weather['timestamp'] = (weather['timestamp'] - pd.to_timedelta(weather['offset'], unit='H'))\n",
    "del weather['offset']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_metadata = pd.read_csv(directory+'/building_metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### open data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(directory+'/sample_submission.csv')\n",
    "df_train = pd.read_csv(directory+'/train.csv', parse_dates=['timestamp'])\n",
    "df_train = df_train.query('not (building_id==1099)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on this great kernel https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem_usg = df.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype != object:  # Exclude strings            \n",
    "            # Print current column type\n",
    "            #print(\"******************************\")\n",
    "            #print(\"Column: \",col)\n",
    "            #print(\"dtype before: \",df[col].dtype)            \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = df[col].max()\n",
    "            mn = df[col].min()\n",
    "            #print(\"min for this col: \",mn)\n",
    "            #print(\"max for this col: \",mx)\n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(df[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                df[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            # test if column can be converted to an integer\n",
    "            asint = df[col].fillna(0).astype(np.int64)\n",
    "            result = (df[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        df[col] = df[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        df[col] = df[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        df[col] = df[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)    \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            \n",
    "            # Print new column type\n",
    "            #print(\"dtype after: \",df[col].dtype)\n",
    "            #print(\"******************************\")\n",
    "    # Print final result\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "    mem_usg = df.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
    "    return df, NAlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 0.0664520263671875  MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  0.024995803833007812  MB\n",
      "This is  37.614810562571755 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "building_metadata, _ = reduce_mem_usage(building_metadata)\n",
    "\n",
    "df_train = df_train.merge(building_metadata, on='building_id', how='left')\n",
    "df_train = df_train.merge(weather, on=['site_id', 'timestamp'], how='left')\n",
    "df_train = df_train.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')\n",
    "df_train = df_train[df_train['air_temperature'].notnull()|df_train['cloud_coverage'].notnull()|df_train['dew_temperature'].notnull()|df_train['precip_depth_1_hr'].notnull()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"hour\"] = df_train[\"timestamp\"].dt.hour\n",
    "df_train[\"weekend\"] = df_train[\"timestamp\"].dt.weekday\n",
    "df_train['year_built'] = df_train['year_built']-1900\n",
    "df_train['square_feet'] = np.log1p(df_train['square_feet'])\n",
    "df_train['meter_reading'] = np.log1p(df_train['meter_reading'])\n",
    "\n",
    "dates_range = pd.date_range(start='2015-12-31', end='2019-01-01')\n",
    "us_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())\n",
    "df_train['is_holiday'] = (df_train['timestamp'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)\n",
    "\n",
    "del df_train[\"timestamp\"]\n",
    "df_train = df_train[~df_train['meter_reading'].isnull()].reset_index(drop=True)\n",
    "df_train['meter_reading'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_train[\"primary_use\"] = le.fit_transform(df_train[\"primary_use\"])\n",
    "\n",
    "target = df_train['meter_reading']\n",
    "del df_train[\"meter_reading\"]\n",
    "\n",
    "df_train = df_train.drop(drop_cols+['is_holiday'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 1836.4669494628906  MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  662.6427917480469  MB\n",
      "This is  36.0824784754144 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "scalers = {}\n",
    "for col in numericals:\n",
    "    ss = StandardScaler()\n",
    "    df_train[col] = ss.fit_transform(df_train[col].values.reshape((-1, 1)))\n",
    "    scalers[col] = ss\n",
    "    \n",
    "df_train, NAlist = reduce_mem_usage(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseNN(dense_dim_1=64, dense_dim_2=32, dense_dim_3=32, dense_dim_4=32, \n",
    "dropout1=0.2, dropout2=0.12, dropout3=0.12, dropout4=0.12, lr=0.001):\n",
    "\n",
    "    #Inputs\n",
    "    site_id = Input(shape=[1], name=\"site_id\")\n",
    "    building_id = Input(shape=[1], name=\"building_id\")\n",
    "    meter = Input(shape=[1], name=\"meter\")\n",
    "    primary_use = Input(shape=[1], name=\"primary_use\")\n",
    "    hour = Input(shape=[1], name=\"hour\")\n",
    "    weekend = Input(shape=[1], name=\"weekend\")\n",
    "    \n",
    "    square_feet = Input(shape=[1], name=\"square_feet\")\n",
    "    year_built = Input(shape=[1], name=\"year_built\")\n",
    "    air_temperature = Input(shape=[1], name=\"air_temperature\")\n",
    "    cloud_coverage = Input(shape=[1], name=\"cloud_coverage\")\n",
    "    dew_temperature = Input(shape=[1], name=\"dew_temperature\")\n",
    "    precip = Input(shape=[1], name=\"precip_depth_1_hr\")\n",
    "    floor_count = Input(shape=[1], name=\"floor_count\")\n",
    "    \n",
    "   \n",
    "    #Embeddings layers\n",
    "    emb_site_id = Embedding(16, 2)(site_id)\n",
    "    emb_building_id = Embedding(1449, 6)(building_id)\n",
    "    emb_meter = Embedding(4, 2)(meter)\n",
    "    emb_primary_use = Embedding(16, 3)(primary_use)\n",
    "    emb_hour = Embedding(24, 3)(hour)\n",
    "    emb_weekend = Embedding(7, 2)(weekend)\n",
    "\n",
    "    concat_emb = concatenate([Flatten() (emb_site_id), \n",
    "                              Flatten() (emb_building_id), \n",
    "                              Flatten() (emb_meter), \n",
    "                              Flatten() (emb_primary_use), \n",
    "                              Flatten() (emb_hour), \n",
    "                              Flatten() (emb_weekend)\n",
    "    ])\n",
    "    \n",
    "    categ = Dropout(dropout1)(Dense(dense_dim_1,activation='relu') (concat_emb))\n",
    "    categ = BatchNormalization()(categ)\n",
    "    categ = Dropout(dropout2)(Dense(dense_dim_2,activation='relu') (categ))\n",
    "    \n",
    "    #main layer\n",
    "    main_l = concatenate([categ, \n",
    "                          square_feet, \n",
    "                          air_temperature, \n",
    "                          cloud_coverage, \n",
    "                          dew_temperature, \n",
    "                          precip, \n",
    "                          year_built, \n",
    "                          floor_count\n",
    "                         ])\n",
    "    \n",
    "    main_l = Dropout(dropout3)(Dense(dense_dim_3,activation='relu') (main_l))\n",
    "    main_l = BatchNormalization()(main_l)\n",
    "    main_l = Dropout(dropout4)(Dense(dense_dim_4,activation='relu') (main_l))\n",
    "    #main_l = BatchNormalization()(main_l)\n",
    "    #output\n",
    "    output = Dense(1, activation='relu') (main_l)\n",
    "\n",
    "    model = Model([ site_id,\n",
    "                    building_id, \n",
    "                    meter, \n",
    "                    primary_use, \n",
    "                    square_feet, \n",
    "                    air_temperature,\n",
    "                    cloud_coverage,\n",
    "                    dew_temperature, \n",
    "                    floor_count,\n",
    "                    year_built,\n",
    "                    hour,\n",
    "                    weekend, \n",
    "                    precip,\n",
    "                  ], output)\n",
    "\n",
    "    model.compile(optimizer = 'adam', #RAdam(learning_rate=0.001),\n",
    "                  loss= 'mse',\n",
    "                  metrics=[rmse])\n",
    "    return model\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_data(df, num_cols, cat_cols):\n",
    "    cols = num_cols + cat_cols\n",
    "    X = {col: np.array(df[col]) for col in cols}\n",
    "    return X\n",
    "\n",
    "def train_model(model, X_t, y_train, batch_size, epochs, X_v, y_valid, fold):\n",
    "    early_stopping = EarlyStopping(patience=7, verbose=0, monitor='val_rmse')\n",
    "    model_checkpoint = ModelCheckpoint(\"model_\" + str(fold) + \".hdf5\",\n",
    "                                       save_best_only=True, verbose=0, monitor='val_rmse', mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_rmse', factor=0.5, patience=3, min_lr=1e-6, verbose=2, mode='min')\n",
    "    hist = model.fit(X_t, y_train, batch_size=batch_size, epochs=epochs,\n",
    "                            validation_data=(X_v, y_valid), verbose=2,\n",
    "                            callbacks=[early_stopping, model_checkpoint, reduce_lr])\n",
    "\n",
    "    model = load_model(\"model_\" + str(fold) + \".hdf5\", custom_objects={'rmse': rmse, 'RAdam':RAdam})\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Train on 14889240 samples, validate on 4963080 samples\n",
      "Epoch 1/10\n",
      " - 271s - loss: 1.2610 - rmse: 1.1025 - val_loss: 1.2211 - val_rmse: 1.0909\n",
      "Epoch 2/10\n",
      " - 265s - loss: 0.9665 - rmse: 0.9821 - val_loss: 1.2041 - val_rmse: 1.0841\n",
      "Epoch 3/10\n",
      " - 263s - loss: 0.9338 - rmse: 0.9653 - val_loss: 1.2071 - val_rmse: 1.0853\n",
      "Epoch 4/10\n",
      " - 263s - loss: 0.9206 - rmse: 0.9585 - val_loss: 1.2059 - val_rmse: 1.0840\n",
      "Epoch 5/10\n",
      " - 264s - loss: 0.9135 - rmse: 0.9547 - val_loss: 1.1989 - val_rmse: 1.0806\n",
      "Epoch 6/10\n",
      " - 261s - loss: 0.9085 - rmse: 0.9521 - val_loss: 1.1922 - val_rmse: 1.0781\n",
      "Epoch 7/10\n",
      " - 261s - loss: 0.9049 - rmse: 0.9502 - val_loss: 1.2004 - val_rmse: 1.0816\n",
      "Epoch 8/10\n",
      " - 262s - loss: 0.9012 - rmse: 0.9483 - val_loss: 1.2078 - val_rmse: 1.0855\n",
      "Epoch 9/10\n",
      " - 261s - loss: 0.8988 - rmse: 0.9470 - val_loss: 1.2018 - val_rmse: 1.0822\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 10/10\n",
      " - 264s - loss: 0.8876 - rmse: 0.9410 - val_loss: 1.2030 - val_rmse: 1.0825\n",
      "**************************************************\n",
      "Fold: 1\n",
      "Train on 14889240 samples, validate on 4963080 samples\n",
      "Epoch 1/10\n",
      " - 281s - loss: 1.2793 - rmse: 1.1031 - val_loss: 1.0750 - val_rmse: 1.0099\n",
      "Epoch 2/10\n",
      " - 276s - loss: 0.9603 - rmse: 0.9790 - val_loss: 1.0877 - val_rmse: 1.0168\n",
      "Epoch 3/10\n",
      " - 274s - loss: 0.9250 - rmse: 0.9609 - val_loss: 1.0832 - val_rmse: 1.0137\n",
      "Epoch 4/10\n",
      " - 271s - loss: 0.9129 - rmse: 0.9545 - val_loss: 1.0696 - val_rmse: 1.0069\n",
      "Epoch 5/10\n",
      " - 270s - loss: 0.9057 - rmse: 0.9507 - val_loss: 1.0738 - val_rmse: 1.0099\n",
      "Epoch 6/10\n",
      " - 275s - loss: 0.9010 - rmse: 0.9483 - val_loss: 1.0687 - val_rmse: 1.0078\n",
      "Epoch 7/10\n",
      " - 276s - loss: 0.8973 - rmse: 0.9462 - val_loss: 1.0889 - val_rmse: 1.0167\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 8/10\n",
      " - 275s - loss: 0.8852 - rmse: 0.9399 - val_loss: 1.0560 - val_rmse: 1.0020\n",
      "Epoch 9/10\n",
      " - 273s - loss: 0.8826 - rmse: 0.9385 - val_loss: 1.0583 - val_rmse: 1.0020\n",
      "Epoch 10/10\n",
      " - 271s - loss: 0.8808 - rmse: 0.9375 - val_loss: 1.0526 - val_rmse: 0.9991\n",
      "**************************************************\n",
      "Fold: 2\n",
      "Train on 14889240 samples, validate on 4963080 samples\n",
      "Epoch 1/10\n",
      " - 280s - loss: 1.2901 - rmse: 1.1152 - val_loss: 1.0347 - val_rmse: 1.0055\n",
      "Epoch 2/10\n",
      " - 279s - loss: 1.0124 - rmse: 1.0053 - val_loss: 1.0275 - val_rmse: 1.0015\n",
      "Epoch 3/10\n",
      " - 279s - loss: 0.9748 - rmse: 0.9864 - val_loss: 1.0708 - val_rmse: 1.0221\n",
      "Epoch 4/10\n",
      " - 273s - loss: 0.9591 - rmse: 0.9785 - val_loss: 1.0760 - val_rmse: 1.0253\n",
      "Epoch 5/10\n",
      " - 273s - loss: 0.9507 - rmse: 0.9741 - val_loss: 1.0436 - val_rmse: 1.0087\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 6/10\n",
      " - 270s - loss: 0.9348 - rmse: 0.9660 - val_loss: 1.0688 - val_rmse: 1.0206\n",
      "Epoch 7/10\n",
      " - 270s - loss: 0.9311 - rmse: 0.9640 - val_loss: 1.0842 - val_rmse: 1.0285\n",
      "Epoch 8/10\n",
      " - 268s - loss: 0.9295 - rmse: 0.9632 - val_loss: 1.0649 - val_rmse: 1.0188\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 9/10\n",
      " - 272s - loss: 0.9225 - rmse: 0.9595 - val_loss: 1.0698 - val_rmse: 1.0208\n",
      "**************************************************\n",
      "Fold: 3\n",
      "Train on 14889240 samples, validate on 4963080 samples\n",
      "Epoch 1/10\n",
      " - 276s - loss: 1.2154 - rmse: 1.0781 - val_loss: 1.3730 - val_rmse: 1.1549\n",
      "Epoch 2/10\n",
      " - 277s - loss: 0.8950 - rmse: 0.9450 - val_loss: 1.3683 - val_rmse: 1.1528\n",
      "Epoch 3/10\n",
      " - 279s - loss: 0.8584 - rmse: 0.9254 - val_loss: 1.3521 - val_rmse: 1.1457\n",
      "Epoch 4/10\n",
      " - 276s - loss: 0.8450 - rmse: 0.9182 - val_loss: 1.3578 - val_rmse: 1.1476\n",
      "Epoch 5/10\n",
      " - 270s - loss: 0.8371 - rmse: 0.9139 - val_loss: 1.3794 - val_rmse: 1.1578\n",
      "Epoch 6/10\n",
      " - 276s - loss: 0.8319 - rmse: 0.9110 - val_loss: 1.3686 - val_rmse: 1.1524\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 7/10\n",
      " - 278s - loss: 0.8179 - rmse: 0.9033 - val_loss: 1.3682 - val_rmse: 1.1530\n",
      "Epoch 8/10\n",
      " - 275s - loss: 0.8150 - rmse: 0.9017 - val_loss: 1.3653 - val_rmse: 1.1516\n",
      "Epoch 9/10\n",
      " - 270s - loss: 0.8136 - rmse: 0.9009 - val_loss: 1.3567 - val_rmse: 1.1483\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 10/10\n",
      " - 273s - loss: 0.8076 - rmse: 0.8976 - val_loss: 1.3613 - val_rmse: 1.1498\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "oof = np.zeros(len(df_train))\n",
    "batch_size = 2048\n",
    "epochs = 10\n",
    "models = []\n",
    "debug = False\n",
    "folds = 4\n",
    "seed = 1024\n",
    "\n",
    "kf = KFold(n_splits=folds, shuffle=False, random_state=seed)\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(kf.split(df_train)):\n",
    "    print('Fold:', fold_n)\n",
    "    X_train, X_valid = df_train.iloc[train_index], df_train.iloc[valid_index]\n",
    "    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n",
    "    X_t = get_keras_data(X_train, numericals, categoricals)\n",
    "    X_v = get_keras_data(X_valid, numericals, categoricals)\n",
    "    \n",
    "    model = DenseNN(dense_dim_1=256, dense_dim_2=64, dense_dim_3=32, dense_dim_4=32, \n",
    "                        dropout1=0.4, dropout2=0.2, dropout3=0.1, dropout4=0.1, lr=0.001)\n",
    "    model = train_model(model, X_t, y_train, batch_size, epochs, X_v, y_valid, fold_n)\n",
    "    oof[valid_index] = np.squeeze(model.predict(X_v))\n",
    "    models.append(model)\n",
    "    print('*'* 50)\n",
    "    if debug:break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['oof'] = oof\n",
    "df_train[['oof']].to_csv('oof_n1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train, target, X_train, X_valid, y_train, y_valid, X_t, X_v, kf\n",
    "gc.collect()\n",
    "\n",
    "test = pd.read_csv(\"../input/ashrae-energy-prediction/test.csv\", parse_dates=['timestamp'])\n",
    "test = test.merge(building_metadata, left_on = \"building_id\", right_on = \"building_id\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.merge(weather, left_on = [\"site_id\", \"timestamp\"], right_on = [\"site_id\", \"timestamp\"], how = \"left\")\n",
    "del weather, building_metadata\n",
    "gc.collect()\n",
    "\n",
    "test[\"hour\"] = test[\"timestamp\"].dt.hour\n",
    "test[\"weekend\"] = test[\"timestamp\"].dt.weekday\n",
    "test['year_built'] = test['year_built']-1900\n",
    "test['square_feet'] = np.log1p(test['square_feet'])\n",
    "test['is_holiday'] = (test['timestamp'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)\n",
    "del test[\"timestamp\"]\n",
    "\n",
    "test[\"primary_use\"] = le.transform(test[\"primary_use\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 4175.422668457031  MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/834 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  1709.9349975585938  MB\n",
      "This is  40.95238095238095 % of the initial size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 834/834 [2:42:05<00:00, 11.66s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>meter_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>175.013351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>96.971977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10.244420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>269.377258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>801.902161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41697595</td>\n",
       "      <td>41697595</td>\n",
       "      <td>8.442062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41697596</td>\n",
       "      <td>41697596</td>\n",
       "      <td>6.668690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41697597</td>\n",
       "      <td>41697597</td>\n",
       "      <td>4.882587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41697598</td>\n",
       "      <td>41697598</td>\n",
       "      <td>161.953339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41697599</td>\n",
       "      <td>41697599</td>\n",
       "      <td>5.879603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41697600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            row_id  meter_reading\n",
       "0                0     175.013351\n",
       "1                1      96.971977\n",
       "2                2      10.244420\n",
       "3                3     269.377258\n",
       "4                4     801.902161\n",
       "...            ...            ...\n",
       "41697595  41697595       8.442062\n",
       "41697596  41697596       6.668690\n",
       "41697597  41697597       4.882587\n",
       "41697598  41697598     161.953339\n",
       "41697599  41697599       5.879603\n",
       "\n",
       "[41697600 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in numericals:\n",
    "    if col not in test:continue\n",
    "    test[col] = scalers[col].transform(test[col].values.reshape((-1, 1)))\n",
    "\n",
    "test = test[feat_cols]\n",
    "test, NAlist = reduce_mem_usage(test)\n",
    "\n",
    "from tqdm import tqdm\n",
    "i=0\n",
    "res=[]\n",
    "step_size = 50000\n",
    "for j in tqdm(range(int(np.ceil(test.shape[0]/50000)))):\n",
    "    for_prediction = get_keras_data(test.iloc[i:i+step_size], numericals, categoricals)\n",
    "    res.append(np.expm1(sum([model.predict(for_prediction) for model in models])/folds))\n",
    "    i+=step_size\n",
    "\n",
    "res = np.concatenate(res)\n",
    "\n",
    "submission = pd.read_csv('/kaggle/input/ashrae-energy-prediction/sample_submission.csv')\n",
    "submission['meter_reading'] = res\n",
    "submission.loc[submission['meter_reading']<0, 'meter_reading'] = 0\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
