{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Collecting meteocalc\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/6c/f7/95473a929f0a02547461fa3698b7f8082ff40445ba5e21601f5d9a5e48ec/meteocalc-1.1.0.tar.gz\n",
      "Building wheels for collected packages: meteocalc\n",
      "  Building wheel for meteocalc (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for meteocalc: filename=meteocalc-1.1.0-cp37-none-any.whl size=8195 sha256=151ea16ceee48f4143a1600742a434b2b804882a5ebf6682eb47fe7e997541b1\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/7c/71/7d/50f6a943a103ae73216168c43b35b9bd5ff25f4f06b15276b4\n",
      "Successfully built meteocalc\n",
      "Installing collected packages: meteocalc\n",
      "Successfully installed meteocalc-1.1.0\n",
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Collecting catboost\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/9a/07/6610e0ae3254d0fc49867d7aa861b78d7b276ecf2ebaddae6be1755966aa/catboost-0.20.1-cp37-none-manylinux1_x86_64.whl (63.6MB)\n",
      "\u001b[K     |████████████████████████████████| 63.6MB 394kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting plotly (from catboost)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/8e/ce/6ea5683c47b682bffad39ad41d10913141b560b1b875a90dbc6abe3f4fa9/plotly-4.4.1-py2.py3-none-any.whl (7.3MB)\n",
      "\u001b[K     |████████████████████████████████| 7.3MB 20kB/s eta 0:00:0101\n",
      "\u001b[?25hRequirement already satisfied: graphviz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from catboost) (0.13)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from catboost) (1.3.0)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from catboost) (1.12.0)\n",
      "Collecting pandas>=0.24.0 (from catboost)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/63/e0/a1b39cdcb2c391f087a1538bc8a6d62a82d0439693192aef541d7b123769/pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4MB)\n",
      "\u001b[K     |██████████████████████▏         | 7.2MB 94kB/s eta 0:00:3401\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py\", line 397, in _error_catcher\n",
      "    yield\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py\", line 479, in read\n",
      "    data = self._fp.read(amt)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/http/client.py\", line 457, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/http/client.py\", line 501, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/ssl.py\", line 1071, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/ssl.py\", line 929, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip/_internal/cli/base_command.py\", line 188, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip/_internal/commands/install.py\", line 345, in run\n",
      "    resolver.resolve(requirement_set)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip/_internal/legacy_resolve.py\", line 196, in resolve\n",
      "    self._resolve_one(requirement_set, req)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip/_internal/legacy_resolve.py\", line 359, in _resolve_one\n",
      "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip/_internal/legacy_resolve.py\", line 307, in _get_abstract_dist_for\n",
      "    self.require_hashes\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip/_internal/operations/prepare.py\", line 199, in prepare_linked_requirement\n",
      "    progress_bar=self.progress_bar\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip/_internal/download.py\", line 1064, in unpack_url\n",
      "    progress_bar=progress_bar\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip/_internal/download.py\", line 924, in unpack_http_url\n",
      "    progress_bar)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip/_internal/download.py\", line 1152, in _download_http_url\n",
      "    _download_url(resp, link, content_file, hashes, progress_bar)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip/_internal/download.py\", line 861, in _download_url\n",
      "    hashes.check_against_chunks(downloaded_chunks)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip/_internal/utils/hashes.py\", line 75, in check_against_chunks\n",
      "    for chunk in chunks:\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip/_internal/download.py\", line 829, in written_chunks\n",
      "    for chunk in chunks:\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip/_internal/utils/ui.py\", line 156, in iter\n",
      "    for x in it:\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip/_internal/download.py\", line 818, in resp_read\n",
      "    decode_content=False):\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py\", line 531, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py\", line 496, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/contextlib.py\", line 130, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py\", line 402, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, 'Read timed out.')\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='mirrors.tuna.tsinghua.edu.cn', port=443): Read timed out.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install meteocalc\n",
    "!pip install catboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from meteocalc import feels_like, Temp\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Collecting catboost\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n",
    "\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\"\n",
    "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!unzip -n /home/aistudio/data/data15386/ashrae.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "building_df = pd.read_csv('building_metadata.csv')\n",
    "weather_df = pd.read_csv('weather_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# eliminate bad rows\n",
    "bad_rows = pd.read_csv('work/rows_to_drop.csv')\n",
    "train_df.drop(bad_rows.loc[:, '0'], inplace = True)\n",
    "train_df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Original code from https://www.kaggle.com/aitude/ashrae-missing-weather-data-handling by @aitude\n",
    "def fill_weather_dataset(weather_df):\n",
    "    \n",
    "    # Find Missing Dates\n",
    "    time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_date = datetime.datetime.strptime(weather_df['timestamp'].min(),time_format)\n",
    "    end_date = datetime.datetime.strptime(weather_df['timestamp'].max(),time_format)\n",
    "    total_hours = int(((end_date - start_date).total_seconds() + 3600) / 3600)\n",
    "    hours_list = [(end_date - datetime.timedelta(hours=x)).strftime(time_format) for x in range(total_hours)]\n",
    "\n",
    "    missing_hours = []\n",
    "    for site_id in range(16):\n",
    "        site_hours = np.array(weather_df[weather_df['site_id'] == site_id]['timestamp'])\n",
    "        new_rows = pd.DataFrame(np.setdiff1d(hours_list,site_hours),columns=['timestamp'])\n",
    "        new_rows['site_id'] = site_id\n",
    "        weather_df = pd.concat([weather_df,new_rows])\n",
    "\n",
    "        weather_df = weather_df.reset_index(drop=True)           \n",
    "\n",
    "    # Add new Features\n",
    "    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"timestamp\"])\n",
    "    weather_df[\"day\"] = weather_df[\"datetime\"].dt.day\n",
    "    weather_df[\"week\"] = weather_df[\"datetime\"].dt.week\n",
    "    weather_df[\"month\"] = weather_df[\"datetime\"].dt.month\n",
    "    \n",
    "    # Reset Index for Fast Update\n",
    "    weather_df = weather_df.set_index(['site_id','day','month'])\n",
    "\n",
    "    air_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\n",
    "    weather_df.update(air_temperature_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    cloud_coverage_filler = weather_df.groupby(['site_id','day','month'])['cloud_coverage'].mean()\n",
    "    # Step 2\n",
    "    cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method='ffill'),columns=[\"cloud_coverage\"])\n",
    "\n",
    "    weather_df.update(cloud_coverage_filler,overwrite=False)\n",
    "\n",
    "    due_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\n",
    "    weather_df.update(due_temperature_filler,overwrite=False)\n",
    "\n",
    "    '''# Step 1\n",
    "    sea_level_filler = weather_df.groupby(['site_id','day','month'])['sea_level_pressure'].mean()\n",
    "    # Step 2\n",
    "    sea_level_filler = pd.DataFrame(sea_level_filler.fillna(method='ffill'),columns=['sea_level_pressure'])\n",
    "\n",
    "    weather_df.update(sea_level_filler,overwrite=False)\n",
    "\n",
    "    wind_direction_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_direction'].mean(),columns=['wind_direction'])\n",
    "    weather_df.update(wind_direction_filler,overwrite=False)\n",
    "\n",
    "    wind_speed_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_speed'].mean(),columns=['wind_speed'])\n",
    "    weather_df.update(wind_speed_filler,overwrite=False)'''\n",
    "\n",
    "    # Step 1\n",
    "    precip_depth_filler = weather_df.groupby(['site_id','day','month'])['precip_depth_1_hr'].mean()\n",
    "    # Step 2\n",
    "    precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method='ffill'),columns=['precip_depth_1_hr'])\n",
    "\n",
    "    weather_df.update(precip_depth_filler,overwrite=False)\n",
    "\n",
    "    weather_df = weather_df.reset_index()\n",
    "    weather_df = weather_df.drop(['datetime','day','week','month'],axis=1)\n",
    "    \n",
    "    def get_meteorological_features(data):\n",
    "        def calculate_rh(df):\n",
    "            df['relative_humidity'] = 100 * (np.exp((17.625 * df['dew_temperature']) / (243.04 + df['dew_temperature'])) / np.exp((17.625 * df['air_temperature'])/(243.04 + df['air_temperature'])))\n",
    "        def calculate_fl(df):\n",
    "            flike_final = []\n",
    "            flike = []\n",
    "            # calculate Feels Like temperature\n",
    "            for i in range(len(df)):\n",
    "                at = df['air_temperature'][i]\n",
    "                rh = df['relative_humidity'][i]\n",
    "                ws = df['wind_speed'][i]\n",
    "                flike.append(feels_like(Temp(at, unit = 'C'), rh, ws))\n",
    "            for i in range(len(flike)):\n",
    "                flike_final.append(flike[i].f)\n",
    "            df['feels_like'] = flike_final\n",
    "            del flike_final, flike, at, rh, ws\n",
    "        calculate_rh(data)\n",
    "        calculate_fl(data)\n",
    "        return data\n",
    "\n",
    "    weather_df = get_meteorological_features(weather_df)\n",
    "    return weather_df.drop(['sea_level_pressure', 'wind_direction', 'wind_speed'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def features_engineering(df):\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df.sort_values(\"timestamp\")\n",
    "    df.reset_index(drop=True)\n",
    "    \n",
    "    # Add more features\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
    "    df[\"dayofweek\"] = df[\"timestamp\"].dt.dayofweek\n",
    "    \n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['month'].replace((1, 2, 3, 4), 1, inplace = True)\n",
    "    df['month'].replace((5, 6, 7, 8), 2, inplace = True)\n",
    "    df['month'].replace((9, 10, 11, 12), 3, inplace = True)\n",
    "  \n",
    "    df['square_feet'] =  np.log1p(df['square_feet'])\n",
    "    \n",
    "    # Remove Unused Columns\n",
    "    drop = [\"timestamp\"]\n",
    "    df = df.drop(drop, axis=1)\n",
    "    gc.collect()\n",
    "    \n",
    "    # Encode Categorical Data\n",
    "    le = LabelEncoder()\n",
    "    df[\"primary_use\"] = le.fit_transform(df[\"primary_use\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:16: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# weather manipulation\n",
    "weather_df = fill_weather_dataset(weather_df)\n",
    "\n",
    "# memory reduction\n",
    "train_df = reduce_mem_usage(train_df,use_float16=True)\n",
    "building_df = reduce_mem_usage(building_df,use_float16=True)\n",
    "weather_df = reduce_mem_usage(weather_df,use_float16=True)\n",
    "\n",
    "# merge data\n",
    "train_df = train_df.merge(building_df, left_on='building_id',right_on='building_id',how='left')\n",
    "train_df = train_df.merge(weather_df,how='left',left_on=['site_id','timestamp'],right_on=['site_id','timestamp'])\n",
    "del weather_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "train_df = features_engineering(train_df)\n",
    "\n",
    "# transform target variable\n",
    "train_df['meter_reading'] = np.log1p(train_df[\"meter_reading\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# declare target, categorical and numeric columns\n",
    "target = 'meter_reading'\n",
    "categorical = ['building_id', 'site_id', 'primary_use', 'meter','dayofweek']\n",
    "numeric_cols = [col for col in train_df.columns if col not in categorical + [target, 'timestamp', 'month']]\n",
    "features = categorical + numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_df.to_pickle('train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "def run_lgbm(train, cat_features = categorical, num_rounds = 20000, folds = 5):\r\n",
    "    kf = StratifiedKFold(n_splits=folds, shuffle=False, random_state=2319)\r\n",
    "    models = []\r\n",
    "\r\n",
    "    oof = np.zeros(len(train))\r\n",
    "    i = 0\r\n",
    "    for tr_idx, val_idx in tqdm(kf.split(train, train['month']), total = folds):\r\n",
    "        \r\n",
    "        tr_x, tr_y = train[features].iloc[tr_idx], train[target].iloc[tr_idx]\r\n",
    "        vl_x, vl_y = train[features].iloc[val_idx], train[target].iloc[val_idx]\r\n",
    "        dtrain = Pool(tr_x, label=tr_y, cat_features=cat_features)\r\n",
    "        dvalid = Pool(vl_x, label=vl_y, cat_features=cat_features) \r\n",
    "        del tr_x, tr_y, vl_y\r\n",
    "        gc.collect()\r\n",
    "        cat_params = {\r\n",
    "        'n_estimators': 3000,#500\r\n",
    "        'learning_rate': 0.05,#0.1\r\n",
    "        'eval_metric': 'RMSE',\r\n",
    "        'loss_function': 'RMSE',\r\n",
    "        'random_seed': 100,\r\n",
    "        'metric_period': 10,\r\n",
    "        'task_type': 'GPU',\r\n",
    "        'depth': 15,\r\n",
    "        }\r\n",
    "        model = CatBoostRegressor(**cat_params)\r\n",
    "        model = model.fit(\r\n",
    "            dtrain, eval_set=dvalid,\r\n",
    "            use_best_model=True,\r\n",
    "            verbose=20,\r\n",
    "            early_stopping_rounds=20)\r\n",
    "        i += 1\r\n",
    "        model.save_model('catboost_fold_%s.bin'%i)\r\n",
    "        models.append(model)\r\n",
    "        oof[val_idx] = model.predict(vl_x)\r\n",
    "        gc.collect()\r\n",
    "    score = np.sqrt(metrics.mean_squared_error(train[target], np.clip(oof, a_min=0, a_max=None)))\r\n",
    "    print('Our oof cv is :', score)\r\n",
    "    \r\n",
    "    return models\r\n",
    "models = run_lgbm(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read test\n",
    "test_df = pd.read_csv('test.csv')\n",
    "row_ids = test_df[\"row_id\"]\n",
    "test_df.drop(\"row_id\", axis=1, inplace=True)\n",
    "test_df = reduce_mem_usage(test_df)\n",
    "\n",
    "# merge with building info\n",
    "test_df = test_df.merge(building_df,left_on='building_id',right_on='building_id',how='left')\n",
    "del building_df\n",
    "gc.collect()\n",
    "\n",
    "# fill test weather data\n",
    "weather_df = pd.read_csv('weather_test.csv')\n",
    "weather_df = fill_weather_dataset(weather_df)\n",
    "weather_df = reduce_mem_usage(weather_df)\n",
    "\n",
    "# merge weather data\n",
    "test_df = test_df.merge(weather_df,how='left',on=['timestamp','site_id'])\n",
    "del weather_df\n",
    "gc.collect()\n",
    "\n",
    "# feature engineering\n",
    "test_df = features_engineering(test_df)\n",
    "#test_df.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split test data into batches\r\n",
    "set_size = len(test_df)\r\n",
    "iterations = 50\r\n",
    "batch_size = set_size // iterations\r\n",
    "\r\n",
    "print(set_size, iterations, batch_size)\r\n",
    "assert set_size == iterations * batch_size\r\n",
    "meter_reading = []\r\n",
    "for i in tqdm(range(iterations)):\r\n",
    "    pos = i*batch_size\r\n",
    "    fold_preds = [np.expm1(model.predict(Pool(test_df[features].iloc[pos : pos+batch_size], cat_features=categorical))) for model in models]\r\n",
    "    meter_reading.extend(np.mean(fold_preds, axis=0))\r\n",
    "\r\n",
    "print(len(meter_reading))\r\n",
    "assert len(meter_reading) == set_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('sample_submission.csv')\r\n",
    "sample_submission['meter_reading'] = np.clip(meter_reading, a_min=0, a_max=None) # clip min at zero\r\n",
    "sample_submission.to_csv('submission.csv', index=False)\r\n",
    "sample_submission.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.6.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
